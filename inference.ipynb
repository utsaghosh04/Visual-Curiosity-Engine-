{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference and Metrics for Curiosity Heatmaps\n",
        "\n",
        "This notebook loads trained models, runs inference on the validation set, and reports:\n",
        "- Pearson correlation\n",
        "- SSIM\n",
        "- MSE\n",
        "- Spearman rank correlation\n",
        "- NDCG@K (configurable)\n",
        "\n",
        "Two models are evaluated:\n",
        "- BLIP-CuriosityNet (vision-only, bbox-target supervision)\n",
        "- VQA-CuriosityNet (vision+language, hybrid-target supervision)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install packages (quiet)\n",
        "%pip install -q numpy scipy scikit-image matplotlib tqdm transformers torch torchvision pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "ilities ready\n"
          ]
        }
      ],
      "source": [
        "# Imports and utilities\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "from transformers import BlipProcessor, BlipModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', DEVICE)\n",
        "\n",
        "# Robust JSON loader\n",
        "\n",
        "def load_json_any_encoding(path: str) -> Dict:\n",
        "    encodings = ['utf-8', 'utf-8-sig', 'utf-16', 'utf-16-le', 'utf-16-be',\n",
        "                 'utf-32', 'utf-32-le', 'utf-32-be', 'cp1252', 'latin1']\n",
        "    raw = Path(path).read_bytes()\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            return json.loads(raw.decode(enc))\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise ValueError(f'Failed to decode JSON: {path}')\n",
        "\n",
        "\n",
        "def load_all_annotations() -> List[Dict]:\n",
        "    all_data: List[Dict] = []\n",
        "    domains = [f'Domain_{i}_Images' for i in range(1, 6)]\n",
        "    for domain_idx, domain_dir in enumerate(domains):\n",
        "        ann_path = Path(domain_dir) / 'annotations.json'\n",
        "        if not ann_path.exists():\n",
        "            continue\n",
        "        try:\n",
        "            data = load_json_any_encoding(str(ann_path))\n",
        "            key = 'annotations' if 'annotations' in data else ('images' if 'images' in data else None)\n",
        "            if key is None:\n",
        "                continue\n",
        "            for img_entry in data[key]:\n",
        "                img_name = img_entry['name']\n",
        "                img_path = Path(domain_dir) / img_name\n",
        "                if not img_path.exists():\n",
        "                    continue\n",
        "                for bbox in img_entry.get('annotations', []):\n",
        "                    attrs = bbox.get('attributes', {})\n",
        "                    all_data.append({\n",
        "                        'image_path': str(img_path),\n",
        "                        'image_name': img_name,\n",
        "                        'domain_id': domain_idx,\n",
        "                        'domain_name': domain_dir,\n",
        "                        'question': (attrs.get('question', '') or '').strip(),\n",
        "                        'question_type': attrs.get('question_type', 'why'),\n",
        "                        'curiosity_score': float(attrs.get('curiosity_score', 0) or 0.0),\n",
        "                        'bbox': {\n",
        "                            'xtl': bbox.get('xtl', 0),\n",
        "                            'ytl': bbox.get('ytl', 0),\n",
        "                            'xbr': bbox.get('xbr', 0),\n",
        "                            'ybr': bbox.get('ybr', 0)\n",
        "                        }\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f'âš  Error loading {domain_dir}: {e}')\n",
        "            continue\n",
        "    return all_data\n",
        "\n",
        "# Metrics\n",
        "\n",
        "def normalize01(arr: np.ndarray) -> np.ndarray:\n",
        "    mn, mx = float(arr.min()), float(arr.max())\n",
        "    if mx - mn < 1e-8:\n",
        "        return np.zeros_like(arr, dtype=np.float32)\n",
        "    return ((arr - mn) / (mx - mn)).astype(np.float32)\n",
        "\n",
        "\n",
        "def metric_pearson(pred: np.ndarray, tgt: np.ndarray) -> float:\n",
        "    p = pred.flatten(); t = tgt.flatten()\n",
        "    if np.std(p) < 1e-8 or np.std(t) < 1e-8:\n",
        "        return 0.0\n",
        "    r, _ = pearsonr(p, t)\n",
        "    return float(r)\n",
        "\n",
        "\n",
        "def metric_spearman(pred: np.ndarray, tgt: np.ndarray) -> float:\n",
        "    p = pred.flatten(); t = tgt.flatten()\n",
        "    if np.std(p) < 1e-8 and np.std(t) < 1e-8:\n",
        "        return 0.0\n",
        "    r, _ = spearmanr(p, t)\n",
        "    return float(0.0 if np.isnan(r) else r)\n",
        "\n",
        "\n",
        "def metric_ssim(pred: np.ndarray, tgt: np.ndarray) -> float:\n",
        "    # SSIM over 2D maps in [0,1]\n",
        "    try:\n",
        "        return float(ssim(pred, tgt, data_range=1.0))\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def metric_mse(pred: np.ndarray, tgt: np.ndarray) -> float:\n",
        "    return float(np.mean((pred - tgt) ** 2))\n",
        "\n",
        "\n",
        "def ndcg_at_k(pred: np.ndarray, tgt: np.ndarray, k: int = 20) -> float:\n",
        "    p = pred.flatten(); t = tgt.flatten()\n",
        "    order = np.argsort(-p)\n",
        "    rel = t[order][:k]\n",
        "    dcg = float(np.sum((2 ** rel - 1) / np.log2(np.arange(2, k + 2))))\n",
        "    ideal_order = np.argsort(-t)\n",
        "    ideal = t[ideal_order][:k]\n",
        "    idcg = float(np.sum((2 ** ideal - 1) / np.log2(np.arange(2, k + 2))))\n",
        "    return float(0.0 if idcg == 0.0 else dcg / idcg)\n",
        "\n",
        "print('Utilities ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets ready\n"
          ]
        }
      ],
      "source": [
        "# Dataset definitions: BLIP (bbox target) and VQA (hybrid target)\n",
        "\n",
        "class DatasetBLIP(Dataset):\n",
        "    def __init__(self, annotations: List[Dict], image_size: int = 224, patch_size: int = 14,\n",
        "                 gaussian_sigma: float = 1.0):\n",
        "        self.ann = annotations\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.gaussian_sigma = gaussian_sigma\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann)\n",
        "\n",
        "    def _bbox_to_patch(self, bbox: Dict, orig_w: int, orig_h: int) -> Tuple[int, int, int, int]:\n",
        "        sx = self.image_size / float(orig_w); sy = self.image_size / float(orig_h)\n",
        "        xtl = int(bbox['xtl'] * sx); ytl = int(bbox['ytl'] * sy)\n",
        "        xbr = int(bbox['xbr'] * sx); ybr = int(bbox['ybr'] * sy)\n",
        "        pw = self.image_size / float(self.patch_size)\n",
        "        px1 = int(xtl / pw); py1 = int(ytl / pw)\n",
        "        px2 = int(xbr / pw); py2 = int(ybr / pw)\n",
        "        px1 = max(0, min(px1, self.patch_size - 1))\n",
        "        py1 = max(0, min(py1, self.patch_size - 1))\n",
        "        px2 = max(0, min(px2, self.patch_size - 1))\n",
        "        py2 = max(0, min(py2, self.patch_size - 1))\n",
        "        return px1, py1, px2, py2\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        a = self.ann[idx]\n",
        "        img = Image.open(a['image_path']).convert('RGB')\n",
        "        orig_w, orig_h = img.size\n",
        "        img_res = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
        "        img_t = torch.from_numpy(np.array(img_res)).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        px1, py1, px2, py2 = self._bbox_to_patch(a['bbox'], orig_w, orig_h)\n",
        "        score01 = float(a.get('curiosity_score', 0.0)) / 5.0\n",
        "\n",
        "        H = self.patch_size\n",
        "        target = torch.zeros(H, H, dtype=torch.float32)\n",
        "        mask = torch.zeros(H, H, dtype=torch.float32)\n",
        "        mask[py1:py2+1, px1:px2+1] = 1.0\n",
        "        target[py1:py2+1, px1:px2+1] = score01\n",
        "        if mask.sum() > 0:\n",
        "            hm = cv2.GaussianBlur(target.numpy(), (0, 0), self.gaussian_sigma)\n",
        "            target = torch.from_numpy(hm.astype(np.float32))\n",
        "\n",
        "        return {\n",
        "            'image': img_t,\n",
        "            'domain_id': torch.tensor(a['domain_id'], dtype=torch.long),\n",
        "            'target_map': target,\n",
        "            'patch_mask': mask,\n",
        "            'image_path': a['image_path'],\n",
        "            'bbox_original': a['bbox'],\n",
        "        }\n",
        "\n",
        "# Hybrid helpers\n",
        "\n",
        "def _spectral_residual_saliency(image_path: str, small_size: int = 256, blur_sigma: int = 3) -> np.ndarray:\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
        "    h, w = gray.shape\n",
        "    g_small = cv2.resize(gray, (small_size, small_size))\n",
        "    F = np.fft.fft2(g_small)\n",
        "    A = np.abs(F); P = np.angle(F)\n",
        "    logA = np.log(A + 1e-8); logA_sm = cv2.blur(logA, (3, 3))\n",
        "    SR = logA - logA_sm\n",
        "    recon = np.fft.ifft2(np.exp(SR + 1j * P))\n",
        "    S = np.abs(recon).astype(np.float32)\n",
        "    S = cv2.GaussianBlur(S, (0, 0), blur_sigma)\n",
        "    S = cv2.resize(S, (w, h))\n",
        "    return normalize01(S)\n",
        "\n",
        "\n",
        "def _gaussian_heatmap(width: int, height: int, anns: List[Dict], sigma_divisor: float = 4.0,\n",
        "                      score_exp: float = 1.5, post_blur: int = 1) -> np.ndarray:\n",
        "    H = np.zeros((height, width), dtype=np.float32)\n",
        "    for ann in anns:\n",
        "        xtl, ytl = float(ann['xtl']), float(ann['ytl'])\n",
        "        xbr, ybr = float(ann['xbr']), float(ann['ybr'])\n",
        "        score = float(ann['attributes'].get('curiosity_score', 0.0))\n",
        "        cx, cy = (xtl + xbr) / 2.0, (ytl + ybr) / 2.0\n",
        "        sx, sy = max((xbr - xtl)/sigma_divisor, 1e-3), max((ybr - ytl)/sigma_divisor, 1e-3)\n",
        "        amp = (score / 5.0) ** score_exp\n",
        "        x0 = int(max(0, cx - 3*sx)); x1 = int(min(width,  cx + 3*sx))\n",
        "        y0 = int(max(0, cy - 3*sy)); y1 = int(min(height, cy + 3*sy))\n",
        "        yy, xx = np.ogrid[y0:y1, x0:x1]\n",
        "        g = amp * np.exp(-(((xx - cx)**2)/(2*sx**2) + ((yy - cy)**2)/(2*sy**2)))\n",
        "        H[y0:y1, x0:x1] = np.maximum(H[y0:y1, x0:x1], g)\n",
        "    if H.max() > 0:\n",
        "        H = normalize01(cv2.GaussianBlur(H, (0,0), post_blur))\n",
        "    return H\n",
        "\n",
        "\n",
        "def _soft_box_mask(width: int, height: int, anns: List[Dict], soft_sigma: int = 8) -> np.ndarray:\n",
        "    M = np.zeros((height, width), dtype=np.float32)\n",
        "    for ann in anns:\n",
        "        xtl = int(ann['xtl']); ytl = int(ann['ytl'])\n",
        "        xbr = int(ann['xbr']); ybr = int(ann['ybr'])\n",
        "        M[ytl:ybr, xtl:xbr] = 1.0\n",
        "    M = cv2.GaussianBlur(M, (0,0), soft_sigma)\n",
        "    return normalize01(M) if M.max() > 0 else M\n",
        "\n",
        "\n",
        "def build_hybrid_heatmap(image_path: str, width: int, height: int, bbox: Dict, score: float,\n",
        "                         alpha: float=0.8, beta: float=0.2, boost: float=2.0,\n",
        "                         lambda_out: float=0.15, sal_small: int=256) -> np.ndarray:\n",
        "    anns = [{'xtl': float(bbox['xtl']), 'ytl': float(bbox['ytl']),\n",
        "             'xbr': float(bbox['xbr']), 'ybr': float(bbox['ybr']),\n",
        "             'attributes': {'curiosity_score': float(score)}}]\n",
        "    G = _gaussian_heatmap(width, height, anns)\n",
        "    S = _spectral_residual_saliency(image_path, small_size=sal_small)\n",
        "    # Build soft mask directly from the provided bbox (robust against collation issues)\n",
        "    M = np.zeros((height, width), dtype=np.float32)\n",
        "    xtl = int(bbox['xtl']); ytl = int(bbox['ytl']); xbr = int(bbox['xbr']); ybr = int(bbox['ybr'])\n",
        "    M[ytl:ybr, xtl:xbr] = 1.0\n",
        "    M = cv2.GaussianBlur(M, (0, 0), 8)\n",
        "    M = normalize01(M) if M.max() > 0 else M\n",
        "    W = lambda_out + (boost - lambda_out) * M\n",
        "    S_att = W * S\n",
        "    C = alpha * G + beta * S_att\n",
        "    return normalize01(C)\n",
        "\n",
        "class DatasetVQA(Dataset):\n",
        "    def __init__(self, annotations: List[Dict], processor: BlipProcessor,\n",
        "                 image_size: int = 224, patch_size: int = 14,\n",
        "                 question_text: str = 'Which parts of the image are likely to provoke human curiosity?'):\n",
        "        self.ann = annotations\n",
        "        self.processor = processor\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.question_text = question_text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann)\n",
        "\n",
        "    def _bbox_to_patch(self, bbox: Dict, orig_w: int, orig_h: int) -> Tuple[int, int, int, int]:\n",
        "        sx = self.image_size / float(orig_w); sy = self.image_size / float(orig_h)\n",
        "        xtl = int(bbox['xtl'] * sx); ytl = int(bbox['ytl'] * sy)\n",
        "        xbr = int(bbox['xbr'] * sx); ybr = int(bbox['ybr'] * sy)\n",
        "        pw = self.image_size / float(self.patch_size)\n",
        "        px1 = int(xtl / pw); py1 = int(ytl / pw)\n",
        "        px2 = int(xbr / pw); py2 = int(ybr / pw)\n",
        "        px1 = max(0, min(px1, self.patch_size - 1))\n",
        "        py1 = max(0, min(py1, self.patch_size - 1))\n",
        "        px2 = max(0, min(px2, self.patch_size - 1))\n",
        "        py2 = max(0, min(py2, self.patch_size - 1))\n",
        "        return px1, py1, px2, py2\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        a = self.ann[idx]\n",
        "        img = Image.open(a['image_path']).convert('RGB')\n",
        "        orig_w, orig_h = img.size\n",
        "        img_res = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
        "        img_t = torch.from_numpy(np.array(img_res)).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        # Hybrid target\n",
        "        C = build_hybrid_heatmap(a['image_path'], orig_w, orig_h, a['bbox'], a.get('curiosity_score', 0.0))\n",
        "        C_14 = cv2.resize(C, (self.patch_size, self.patch_size), interpolation=cv2.INTER_LINEAR)\n",
        "        target = torch.from_numpy(C_14.astype(np.float32))\n",
        "\n",
        "        # Patch mask from bbox\n",
        "        px1, py1, px2, py2 = self._bbox_to_patch(a['bbox'], orig_w, orig_h)\n",
        "        mask = torch.zeros(self.patch_size, self.patch_size, dtype=torch.float32)\n",
        "        mask[py1:py2+1, px1:px2+1] = 1.0\n",
        "\n",
        "        text_inputs = self.processor.tokenizer(\n",
        "            self.question_text, padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'image': img_t,\n",
        "            'input_ids': text_inputs.input_ids.squeeze(0),\n",
        "            'attention_mask': text_inputs.attention_mask.squeeze(0),\n",
        "            'target_map': target,\n",
        "            'patch_mask': mask,\n",
        "            'image_path': a['image_path'],\n",
        "            'bbox_original': a['bbox'],\n",
        "            'question_type': a.get('question_type', 'why')\n",
        "        }\n",
        "\n",
        "print(' Datasets ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models ready\n"
          ]
        }
      ],
      "source": [
        "# Model definitions\n",
        "\n",
        "class BLIPCuriosityNet(nn.Module):\n",
        "    def __init__(self, num_domains: int = 5, hidden_dim: int = 768, freeze_vision: bool = True):\n",
        "        super().__init__()\n",
        "        self.blip = BlipModel.from_pretrained('Salesforce/blip-image-captioning-base')\n",
        "        self.vision = self.blip.vision_model\n",
        "        if freeze_vision:\n",
        "            for p in self.vision.parameters():\n",
        "                p.requires_grad = False\n",
        "        self.domain_embedding = nn.Embedding(num_domains, 128)\n",
        "        self.domain_proj = nn.Linear(128, hidden_dim)\n",
        "        # Match training name to load weights correctly\n",
        "        self.curiosity_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, images: torch.Tensor, domain_ids: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.vision(pixel_values=images)\n",
        "        feats = out.last_hidden_state[:, 1:, :]  # [B,196,D]\n",
        "        d = self.domain_proj(self.domain_embedding(domain_ids)).unsqueeze(1)\n",
        "        feats = feats + d\n",
        "        scores = self.curiosity_head(feats).squeeze(-1)  # [B,196]\n",
        "        B = images.size(0)\n",
        "        return scores.view(B, 14, 14)\n",
        "\n",
        "class VQACuriosityNet(nn.Module):\n",
        "    def __init__(self, hidden_dim: int = 768, patch_size: int = 14, freeze_vision: bool = True, freeze_text: bool = True):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.blip = BlipModel.from_pretrained('Salesforce/blip-image-captioning-base')\n",
        "        self.vision = self.blip.vision_model\n",
        "        self.text = self.blip.text_model\n",
        "        if freeze_vision:\n",
        "            for p in self.vision.parameters():\n",
        "                p.requires_grad = False\n",
        "        if freeze_text:\n",
        "            for p in self.text.parameters():\n",
        "                p.requires_grad = False\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.scale = hidden_dim ** 0.5\n",
        "        # Match training names\n",
        "        self.fusion_ln = nn.LayerNorm(hidden_dim)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, images: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        V = self.vision(pixel_values=images).last_hidden_state[:, 1:, :]  # [B,196,D]\n",
        "        T = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # [B,L,D]\n",
        "        Q = self.q_proj(V); K = self.k_proj(T); Vt = self.v_proj(T)\n",
        "        attn = torch.matmul(Q, K.transpose(1, 2)) / self.scale\n",
        "        w = torch.softmax(attn, dim=-1)\n",
        "        attn_txt = torch.matmul(w, Vt)\n",
        "        fused = self.fusion_ln(V + attn_txt)\n",
        "        scores = self.decoder(fused).squeeze(-1)  # [B,196]\n",
        "        B = images.size(0)\n",
        "        return scores.view(B, self.patch_size, self.patch_size)\n",
        "\n",
        "print(' Models ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val sizes -> BLIP: 44 | VQA: 44\n"
          ]
        }
      ],
      "source": [
        "# Build datasets and loaders (validation split only)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_annotations = load_all_annotations()\n",
        "train_ann, val_ann = train_test_split(all_annotations, test_size=0.2, random_state=42)\n",
        "\n",
        "processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')\n",
        "\n",
        "val_blip = DatasetBLIP(val_ann, image_size=224, patch_size=14)\n",
        "val_vqa  = DatasetVQA(val_ann, processor=processor, image_size=224, patch_size=14)\n",
        "\n",
        "loader_blip = DataLoader(val_blip, batch_size=8, shuffle=False, num_workers=0)\n",
        "loader_vqa  = DataLoader(val_vqa,  batch_size=6, shuffle=False, num_workers=0)\n",
        "\n",
        "print('Val sizes -> BLIP:', len(val_blip), '| VQA:', len(val_vqa))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
            "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded BLIP weights\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`BlipModel` is going to be deprecated in future release, please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n",
            "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded VQA weights\n"
          ]
        }
      ],
      "source": [
        "# Load weights\n",
        "blip_ckpt = 'blip_curiosity_net_best.pth'\n",
        "vqa_ckpt  = 'vqa_curiosity_best.pth'\n",
        "\n",
        "model_blip = BLIPCuriosityNet().to(DEVICE)\n",
        "if Path(blip_ckpt).exists():\n",
        "    sd = torch.load(blip_ckpt, map_location=DEVICE)\n",
        "    state = sd.get('model_state_dict', sd)\n",
        "    model_blip.load_state_dict(state, strict=False)\n",
        "    print(' Loaded BLIP weights')\n",
        "else:\n",
        "    print(' BLIP checkpoint not found:', blip_ckpt)\n",
        "\n",
        "model_vqa = VQACuriosityNet().to(DEVICE)\n",
        "if Path(vqa_ckpt).exists():\n",
        "    sd = torch.load(vqa_ckpt, map_location=DEVICE)\n",
        "    state = sd.get('model_state_dict', sd)\n",
        "    model_vqa.load_state_dict(state, strict=False)\n",
        "    print(' Loaded VQA weights')\n",
        "else:\n",
        "    print(' VQA checkpoint not found:', vqa_ckpt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ference helpers ready\n"
          ]
        }
      ],
      "source": [
        "# Inference helpers\n",
        "\n",
        "def evaluate_loader_heatmaps(pred_maps: List[np.ndarray], tgt_maps: List[np.ndarray], ndcg_ks: List[int]):\n",
        "    metrics = {\n",
        "        'pearson': [], 'spearman': [], 'ssim': [], 'mse': [],\n",
        "        **{f'ndcg@{k}': [] for k in ndcg_ks}\n",
        "    }\n",
        "    for pred, tgt in zip(pred_maps, tgt_maps):\n",
        "        p = normalize01(pred); t = normalize01(tgt)\n",
        "        metrics['pearson'].append(metric_pearson(p, t))\n",
        "        metrics['spearman'].append(metric_spearman(p, t))\n",
        "        metrics['ssim'].append(metric_ssim(p, t))\n",
        "        metrics['mse'].append(metric_mse(p, t))\n",
        "        for k in ndcg_ks:\n",
        "            metrics[f'ndcg@{k}'].append(ndcg_at_k(p, t, k=k))\n",
        "    return {k: float(np.mean(v)) for k, v in metrics.items()}, metrics\n",
        "\n",
        "\n",
        "def run_inference_blip(model: nn.Module, loader: DataLoader, ndcg_ks: List[int]):\n",
        "    model.eval()\n",
        "    preds, tgts = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='BLIP inference'):\n",
        "            images = batch['image'].to(DEVICE)\n",
        "            domain = batch['domain_id'].to(DEVICE)\n",
        "            out = model(images, domain).cpu().numpy()\n",
        "            tgt = batch['target_map'].numpy()\n",
        "            preds.extend(list(out)); tgts.extend(list(tgt))\n",
        "    return evaluate_loader_heatmaps(preds, tgts, ndcg_ks)\n",
        "\n",
        "\n",
        "def run_inference_vqa(model: nn.Module, loader: DataLoader, ndcg_ks: List[int]):\n",
        "    model.eval()\n",
        "    preds, tgts = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='VQA inference'):\n",
        "            images = batch['image'].to(DEVICE)\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            out = model(images, input_ids, attention_mask).cpu().numpy()\n",
        "            tgt = batch['target_map'].numpy()\n",
        "            preds.extend(list(out)); tgts.extend(list(tgt))\n",
        "    return evaluate_loader_heatmaps(preds, tgts, ndcg_ks)\n",
        "\n",
        "print('Inference helpers ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BLIP inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:15<00:00,  2.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLIP mean metrics: {'pearson': 0.4224326319315217, 'spearman': 0.37843533846782956, 'ssim': 0.33373851862700976, 'mse': 0.18075483969666742, 'ndcg@5': 0.53326053906122, 'ndcg@10': 0.5447952676519591, 'ndcg@20': 0.5602379274741165}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VQA inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:24<00:00,  3.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VQA mean metrics: {'pearson': 0.43619992617856373, 'spearman': 0.532549357821294, 'ssim': 0.20362672629086506, 'mse': 0.22387673180889, 'ndcg@5': 0.40532851549812654, 'ndcg@10': 0.46368258540464624, 'ndcg@20': 0.53055352894218}\n",
            "ved metrics to inference_outputs\\metrics.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run inference and gather metrics\n",
        "\n",
        "NDCG_KS = [5, 10, 20]\n",
        "\n",
        "summary = {}\n",
        "\n",
        "blip_mean, blip_all = run_inference_blip(model_blip, loader_blip, NDCG_KS)\n",
        "summary['BLIP-CuriosityNet'] = blip_mean\n",
        "print('BLIP mean metrics:', blip_mean)\n",
        "\n",
        "vqa_mean, vqa_all = run_inference_vqa(model_vqa, loader_vqa, NDCG_KS)\n",
        "summary['VQA-CuriosityNet'] = vqa_mean\n",
        "print('VQA mean metrics:', vqa_mean)\n",
        "\n",
        "# Save\n",
        "outdir = Path('inference_outputs'); outdir.mkdir(exist_ok=True)\n",
        "with open(outdir / 'metrics.json', 'w') as f:\n",
        "    json.dump({'summary': summary, 'ndcg_ks': NDCG_KS, 'blip_all': blip_all, 'vqa_all': vqa_all}, f, indent=2)\n",
        "print('Saved metrics to', outdir / 'metrics.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved overlays for img_061.png â†’ inference_outputs\\vis\n",
            "Saved overlays for img_024.png â†’ inference_outputs\\vis\n",
            "Saved overlays for img_060.png â†’ inference_outputs\\vis\n",
            "Saved overlays for img_166.png â†’ inference_outputs\\vis\n",
            "Saved overlays for img_154.png â†’ inference_outputs\\vis\n"
          ]
        }
      ],
      "source": [
        "# Visualize predicted overlays for specific images (both models)\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "def save_overlay(img_np: np.ndarray, heatmap_up: np.ndarray, save_path: Path, alpha: float = 0.6):\n",
        "    hm = normalize01(heatmap_up)\n",
        "    hot = cm.get_cmap('hot')\n",
        "    hm_rgb = (hot(hm)[..., :3] * 255).astype(np.uint8)\n",
        "    blend = np.clip(alpha * hm_rgb + (1 - alpha) * img_np, 0, 255).astype(np.uint8)\n",
        "    plt.imsave(str(save_path), blend)\n",
        "\n",
        "names = ['img_061.png', 'img_024.png', 'img_060.png', 'img_166.png' , 'img_154.png']\n",
        "out_vis = Path('inference_outputs/vis'); out_vis.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "prompt = 'Which parts of the image are likely to provoke human curiosity?'\n",
        "\n",
        "for name in names:\n",
        "    # Find annotation entry (first match)\n",
        "    ann = next((a for a in all_annotations if a['image_name'].lower() == name.lower()), None)\n",
        "    if ann is None:\n",
        "        print(f'âš  Not found in annotations: {name}');\n",
        "        continue\n",
        "\n",
        "    img_path = ann['image_path']\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    W, H = img.size\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    # Prepare image tensor\n",
        "    img_res = img.resize((224, 224), Image.BILINEAR)\n",
        "    img_t = torch.from_numpy(np.array(img_res)).permute(2, 0, 1).float() / 255.0\n",
        "    img_t = img_t.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # BLIP prediction\n",
        "    with torch.no_grad():\n",
        "        d = torch.tensor([ann.get('domain_id', 0)], dtype=torch.long, device=DEVICE)\n",
        "        pred_blip = model_blip(img_t, d).cpu().numpy()[0]\n",
        "    pred_blip_up = cv2.resize(pred_blip, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "    save_overlay(img_np, pred_blip_up, out_vis / f'BLIP_{name.replace(\".png\", \"\")}_overlay.png')\n",
        "\n",
        "    # VQA prediction\n",
        "    with torch.no_grad():\n",
        "        token = processor.tokenizer(prompt, padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
        "        inp_ids = token.input_ids.to(DEVICE)\n",
        "        attn = token.attention_mask.to(DEVICE)\n",
        "        pred_vqa = model_vqa(img_t, inp_ids, attn).cpu().numpy()[0]\n",
        "    pred_vqa_up = cv2.resize(pred_vqa, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "    save_overlay(img_np, pred_vqa_up, out_vis / f'VQA_{name.replace(\".png\", \"\")}_overlay.png')\n",
        "\n",
        "    print(f'Saved overlays for {name} â†’', out_vis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 14Ã—14 arrays and images for img_061.png â†’ inference_outputs\\heatmaps14\n",
            "Saved 14Ã—14 arrays and images for img_024.png â†’ inference_outputs\\heatmaps14\n",
            "Saved 14Ã—14 arrays and images for img_060.png â†’ inference_outputs\\heatmaps14\n",
            "Saved 14Ã—14 arrays and images for img_166.png â†’ inference_outputs\\heatmaps14\n",
            "Saved 14Ã—14 arrays and images for img_154.png â†’ inference_outputs\\heatmaps14\n"
          ]
        }
      ],
      "source": [
        "# Save raw 14Ã—14 heatmaps (BLIP & VQA) for selected images\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "names = ['img_061.png', 'img_024.png', 'img_060.png', 'img_166.png' , 'img_154.png']\n",
        "out14 = Path('inference_outputs/heatmaps14'); out14.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for name in names:\n",
        "    ann = next((a for a in all_annotations if a['image_name'].lower() == name.lower()), None)\n",
        "    if ann is None:\n",
        "        print(f'âš  Not found in annotations: {name}');\n",
        "        continue\n",
        "\n",
        "    # Prepare image tensor\n",
        "    img = Image.open(ann['image_path']).convert('RGB')\n",
        "    img_res = img.resize((224, 224), Image.BILINEAR)\n",
        "    img_t = torch.from_numpy(np.array(img_res)).permute(2, 0, 1).float() / 255.0\n",
        "    img_t = img_t.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # BLIP 14Ã—14\n",
        "    with torch.no_grad():\n",
        "        d = torch.tensor([ann.get('domain_id', 0)], dtype=torch.long, device=DEVICE)\n",
        "        blip_14 = model_blip(img_t, d).cpu().numpy()[0]\n",
        "    np.save(out14 / f'BLIP_{name.replace(\".png\", \"\")}_14.npy', blip_14)\n",
        "    np.savetxt(out14 / f'BLIP_{name.replace(\".png\", \"\")}_14.csv', blip_14, delimiter=',', fmt='%.6f')\n",
        "    plt.imsave(out14 / f'BLIP_{name.replace(\".png\", \"\")}_14_hot.png', normalize01(blip_14), cmap='hot')\n",
        "\n",
        "    # VQA 14Ã—14\n",
        "    with torch.no_grad():\n",
        "        token = processor.tokenizer('Which parts of the image are likely to provoke human curiosity?',\n",
        "                                    padding='max_length', truncation=True, max_length=32, return_tensors='pt')\n",
        "        inp_ids = token.input_ids.to(DEVICE)\n",
        "        attn = token.attention_mask.to(DEVICE)\n",
        "        vqa_14 = model_vqa(img_t, inp_ids, attn).cpu().numpy()[0]\n",
        "    np.save(out14 / f'VQA_{name.replace(\".png\", \"\")}_14.npy', vqa_14)\n",
        "    np.savetxt(out14 / f'VQA_{name.replace(\".png\", \"\")}_14.csv', vqa_14, delimiter=',', fmt='%.6f')\n",
        "    plt.imsave(out14 / f'VQA_{name.replace(\".png\", \"\")}_14_hot.png', normalize01(vqa_14), cmap='hot')\n",
        "\n",
        "    print(f'Saved 14Ã—14 arrays and images for {name} â†’', out14)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
